{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba715fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280e350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 07:36:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "os.environ['SPARK_HOME'] = '/usr/local/lib/python3.9/site-packages/pyspark'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"np-demo-restapi-pyspark\")\n",
    "  .config(\"spark.default.parallelism\",4)\n",
    "  .getOrCreate())\n",
    "\n",
    "# spark.conf.set(\"spark.default.parallelism\",4)\n",
    "\n",
    "print('Init done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc416e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- products: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_full_path = '/Users/nguyenpham/Downloads/np-demo-restapi-pyspark/20190207_transactions.json'\n",
    "\n",
    "df = (spark.read.json(file_full_path))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19c9037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "921ec293",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1263b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa49ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "explode_df = part_df.select(part_df.id,explode(part_df.products)).withColumnRenamed('col', 'product_id')\n",
    "explode_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d95e6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [id#7L, col#18L AS product_id#21L]\n",
      "+- Generate explode(products#8), [id#7L], false, [col#18L]\n",
      "   +- Exchange RoundRobinPartitioning(4), REPARTITION_WITH_NUM, [id=#60]\n",
      "      +- *(1) Filter ((size(products#8, true) > 0) AND isnotnull(products#8))\n",
      "         +- FileScan json [id#7L,products#8] Batched: false, DataFilters: [(size(products#8, true) > 0), isnotnull(products#8)], Format: JSON, Location: InMemoryFileIndex[file:/Users/nguyenpham/Downloads/np-demo-restapi-pyspark/20190207_transactions...., PartitionFilters: [], PushedFilters: [IsNotNull(products)], ReadSchema: struct<id:bigint,products:array<bigint>>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_explode.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f85847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+----------+-----+\n",
      "|product_id|count|\n",
      "+----------+-----+\n",
      "|        29|  112|\n",
      "|        26|  101|\n",
      "|        65|  108|\n",
      "|       191|  113|\n",
      "|       222|   91|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df = explode_df.groupBy('product_id').count()\n",
    "count_df.printSchema()\n",
    "count_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6ea72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29ce94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[product_id#29L], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(product_id#29L, 200), ENSURE_REQUIREMENTS, [id=#166]\n",
      "   +- *(2) HashAggregate(keys=[product_id#29L], functions=[partial_count(1)])\n",
      "      +- *(2) Project [col#26L AS product_id#29L]\n",
      "         +- Generate explode(products#8), false, [col#26L]\n",
      "            +- Exchange RoundRobinPartitioning(4), REPARTITION_WITH_NUM, [id=#160]\n",
      "               +- *(1) Filter ((size(products#8, true) > 0) AND isnotnull(products#8))\n",
      "                  +- FileScan json [products#8] Batched: false, DataFilters: [(size(products#8, true) > 0), isnotnull(products#8)], Format: JSON, Location: InMemoryFileIndex[file:/Users/nguyenpham/Downloads/np-demo-restapi-pyspark/20190207_transactions...., PartitionFilters: [], PushedFilters: [IsNotNull(products)], ReadSchema: struct<products:array<bigint>>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b8a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
